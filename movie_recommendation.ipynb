{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f24d0f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import precision_score , recall_score , f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba345d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "movies_metadata = pd.read_csv('movies_metadata.csv', low_memory=False)\n",
    "keywords = pd.read_csv('keywords.csv')\n",
    "credits = pd.read_csv('credits.csv')\n",
    "links = pd.read_csv('links.csv')\n",
    "links_small = pd.read_csv('links_small.csv')\n",
    "ratings_small = pd.read_csv('ratings_small.csv')\n",
    "ratings = pd.read_csv('ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4199cd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "movies_metadata = movies_metadata.dropna(subset=['title', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e7daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ids to integers so that data can be merged\n",
    "movies_metadata['id'] = movies_metadata['id'].astype(int)\n",
    "keywords['id'] = keywords['id'].astype(int)\n",
    "credits['id'] = credits['id'].astype(int)\n",
    "\n",
    "# Merging datasets on 'id'\n",
    "merged_data = movies_metadata.merge(credits, on='id').merge(keywords, on='id')\n",
    "\n",
    "# Merging with links also on 'id'\n",
    "merged_data = merged_data.merge(links, left_on='id', right_on='tmdbId')\n",
    "\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e97d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example EDA\n",
    "print(movies_metadata.info()) \n",
    "#This provides a concise summary of the DataFrame, including the number of non-null entries in each column,\n",
    "# the data type of each column, and the memory usage of the DataFrame which helps to define what models can ml models can be applied on it , as they all expect different datatypes \n",
    "print(movies_metadata.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfab022e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'movies_metadata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##if the else condition works , I know there is something wrong with my preprocessing\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#helps us to visualise how much revenue and runtime has been generated by each movie . It gives us an idea of data distribution  \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrevenue\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m movies_metadata\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruntime\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m movies_metadata\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'movies_metadata' is not defined"
     ]
    }
   ],
   "source": [
    "##if the else condition works , I know there is something wrong with my preprocessing\n",
    "#helps us to visualise how much revenue and runtime has been generated by each movie . It gives us an idea of data distribution  \n",
    "if 'revenue' in movies_metadata.columns and 'runtime' in movies_metadata.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(movies_metadata['revenue'])\n",
    "    plt.xlabel('Revenue (USD)')\n",
    "    plt.ylabel('Number of movies')\n",
    "    plt.title('Distribution of Movie Revenue')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(movies_metadata['runtime'])\n",
    "    plt.xlabel('Runtime (minutes)')\n",
    "    plt.ylabel('Number of movies')\n",
    "    plt.title('Distribution of Movie Runtime')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Columns 'revenue' or 'runtime' are missing. Skipping histograms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de63ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average vote average by genre (if applicable) , helps in ranking the highest watched genre . \n",
    "#We can give more weighhtage to this genre while recommending movies \n",
    "if 'genres' in movies_metadata.columns and 'vote_average' in movies_metadata.columns:\n",
    "    # Impute missing values (replace with your preferred imputation method)\n",
    "    imputed_movies_metadata = movies_metadata.fillna(method='ffill')  # Example using forward fill\n",
    "    # Group by genres and calculate average vote\n",
    "    genre_groups = imputed_movies_metadata.groupby('genres')['vote_average'].mean()\n",
    "    print(\"\\nAverage vote average by genre:\")\n",
    "    print(genre_groups.sort_values(ascending=False))  # Sort by highest average\n",
    "    # Find the movie with the most votes\n",
    "    movie_with_most_votes = imputed_movies_metadata.loc[imputed_movies_metadata['vote_count'].idxmax()]\n",
    "\n",
    "else:\n",
    "    print(\"Columns 'genres' or 'vote_average' are missing. Skipping average vote average by genre.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628ab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate popularity score based on vote counts\n",
    "popularity_score = ratings.groupby('movieId')['rating'].count().reset_index(name='popularity')\n",
    "\n",
    "# Merge popularity score with ratings data\n",
    "ratings = ratings.merge(popularity_score, on='movieId')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1578bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic sentiment analysis on overview so that moovies with more poaitive reviews get more weightage while prediction \n",
    "#also didnt use bayesian , becuase it is too complex and time consuming for just a simple sentiment analysis \n",
    "def basic_sentiment(text):\n",
    "  blob = TextBlob(text)  # Pass the text argument\n",
    "  sentiment = blob.sentiment.polarity\n",
    "  if sentiment > 0:\n",
    "    return 'Positive'\n",
    "  elif sentiment < 0:\n",
    "    return 'Negative'\n",
    "  else:\n",
    "    return 'Neutral'\n",
    "\n",
    "\n",
    "#Ensure 'overview' column has no NaN values by filling them with an empty string ensuring\n",
    "#that every entry in the overview column is a string, which avoids the TypeError when TextBlob processes the text.\n",
    "movies_metadata['overview'] = movies_metadata['overview'].fillna('')\n",
    "movies_metadata['sentiment'] = movies_metadata['overview'].apply(basic_sentiment)\n",
    "sentiment_map = {'Positive': 1, 'Neutral': 0, 'Negative': -1}\n",
    "movies_metadata['sentiment_score'] = movies_metadata['sentiment'].map(sentiment_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c5103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing rating distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(ratings_small['rating'], bins=10, kde=True)\n",
    "plt.title('Rating Distribution')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the main cast\n",
    "def get_main_cast(cast):\n",
    "    return ', '.join([actor['name'] for actor in eval(cast)[:3]])\n",
    "\n",
    "# Function to get the director\n",
    "def get_director(crew):\n",
    "    for member in eval(crew):\n",
    "        if member['job'] == 'Director':\n",
    "            return member['name']\n",
    "    return ''\n",
    "\n",
    "# Create a new column for main cast and director\n",
    "merged_data['main_cast'] = merged_data['cast'].apply(get_main_cast)\n",
    "merged_data['director'] = merged_data['crew'].apply(get_director)\n",
    "\n",
    "# Combine genres, keywords, main cast, and director into a single string\n",
    "merged_data['combined_features'] = merged_data.apply(\n",
    "    lambda x: ' '.join(x['genres'] + ' ' + x['keywords'] + ' ' + x['main_cast'] + ' ' + x['director']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e8338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using content filtering \n",
    "def content_filtering(merged_data):\n",
    "    # Create TF-IDF matrix for keywords\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    merged_data['combined_features'] = merged_data['combined_features'].fillna('')\n",
    "    tfidf_matrix = tfidf.fit_transform(merged_data['combined_features'])\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "    # Integrate sentiment scores into the cosine similarity matrix\n",
    "    sentiment_scores = movies_metadata['sentiment_score'].values\n",
    "    adjusted_cosine_sim = cosine_sim * sentiment_scores[:, np.newaxis]\n",
    "\n",
    "    # Plotting the similarity matrix as a heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(adjusted_cosine_sim, cmap='viridis')\n",
    "    plt.title('Movie Similarity Matrix')\n",
    "    plt.xlabel('Movies')\n",
    "    plt.ylabel('Movies')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "all_recommendations = content_filtering(merged_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf15f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#using only collabarative filtering \n",
    "# Example of k-NN for collaborative filtering\n",
    "\n",
    "# Define collaborative filtering function\n",
    "def collaborative_filtering(ratings, n_recommendations=5):\n",
    "    # Create the user-item interaction matrix\n",
    "    ratings_pivot = ratings.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "    \n",
    "    # Initialize and fit the k-NN model\n",
    "    model_knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "    model_knn.fit(ratings_pivot.values)\n",
    "    \n",
    "    # Store the movie ids and their respective indices\n",
    "    movie_ids = ratings_pivot.columns.tolist()\n",
    "    movie_indices = {movie_id: idx for idx, movie_id in enumerate(movie_ids)}\n",
    "    \n",
    "    all_recommendations = {}\n",
    "    \n",
    "    # Iterate over all movies\n",
    "    for movie_id in movie_ids:\n",
    "        # Get the index of the movie\n",
    "        movie_idx = movie_indices[movie_id]\n",
    "        \n",
    "        # Get the k-nearest neighbors for the movie\n",
    "        distances, indices = model_knn.kneighbors(ratings_pivot.values[:, movie_idx].reshape(1, -1), n_neighbors=n_recommendations+1)\n",
    "        \n",
    "        # Get the indices of the nearest neighbors (excluding the movie itself)\n",
    "        similar_indices = indices.flatten()[1:]\n",
    "        \n",
    "        # Map indices back to movie ids\n",
    "        similar_movie_ids = [movie_ids[i] for i in similar_indices]\n",
    "        \n",
    "        # Store the recommendations\n",
    "        all_recommendations[movie_id] = similar_movie_ids\n",
    "    \n",
    "    return all_recommendations\n",
    "\n",
    "# Example usage\n",
    "all_recommendations = collaborative_filtering(ratings)\n",
    "print(all_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd0600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using both content and collabarative filtering \n",
    "\n",
    "def get_recommendations(title, cosine_sim=adjusted_cosine_sim):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = merged_data[merged_data['title'] == title].index[0]\n",
    "\n",
    "    # Get the pairwise similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 5 most similar movies\n",
    "    sim_scores = sim_scores[1:6]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "\n",
    "    # Get popularity scores of recommended movies\n",
    "    popularity = ratings.groupby('movieId')['popularity'].max()  # Calculate popularity again\n",
    "    popularity_scores = popularity.iloc[movie_indices]['popularity'].values\n",
    "\n",
    "    # Adjust similarity scores with popularity scores\n",
    "    sim_scores_popularity_adjusted = [sim_scores[i][1] * popularity_scores[i] for i in range(len(sim_scores))]\n",
    "\n",
    "    # Sort the adjusted scores\n",
    "    sim_scores_popularity_adjusted = sorted(zip(movie_indices, sim_scores_popularity_adjusted), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top 5 most similar movies with popularity adjustment\n",
    "    top_movies = [merged_data.iloc[score[0]]['title'] for score in sim_scores_popularity_adjusted[:5]]\n",
    "\n",
    "    return top_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e207ed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using both content and collabarative filtering \n",
    "def get_recommendations_for_all(merged_data,ratings):\n",
    "    \n",
    "    # Create TF-IDF matrix for keywords\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    merged_data['combined_features'] = merged_data['combined_features'].fillna('')\n",
    "    tfidf_matrix = tfidf.fit_transform(merged_data['combined_features'])\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "    # Integrate sentiment scores into the cosine similarity matrix\n",
    "    sentiment_scores = movies_metadata['sentiment_score'].values\n",
    "    adjusted_cosine_sim = cosine_sim * sentiment_scores[:, np.newaxis]\n",
    "\n",
    "    all_recommendations = {}\n",
    "\n",
    "    # Iterate over all movie titles\n",
    "    for idx in range(len(merged_data)):\n",
    "        title = merged_data.iloc[idx]['title']\n",
    "        \n",
    "        # Get the pairwise similarity scores of all movies with that movie\n",
    "        sim_scores = list(enumerate(adjusted_cosine_sim[idx]))\n",
    "\n",
    "        # Sort the movies based on the similarity scores\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get the scores of the 5 most similar movies\n",
    "        sim_scores = sim_scores[1:6]\n",
    "\n",
    "        # Get the movie indices\n",
    "        movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "        # Get popularity scores of recommended movies\n",
    "        popularity = ratings.groupby('movieId')['popularity'].max()  # Calculate popularity again\n",
    "        popularity_scores = popularity.iloc[movie_indices]['popularity'].values\n",
    "\n",
    "        # Adjust similarity scores with popularity scores\n",
    "        sim_scores_popularity_adjusted = [sim_scores[i][1] * popularity_scores[i] for i in range(len(sim_scores))]\n",
    "\n",
    "        # Sort the adjusted scores\n",
    "        sim_scores_popularity_adjusted = sorted(zip(movie_indices, sim_scores_popularity_adjusted), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get the top 5 most similar movies with popularity adjustment\n",
    "        top_movies = [merged_data.iloc[score[0]]['title'] for score in sim_scores_popularity_adjusted[:5]]\n",
    "\n",
    "        # Store recommendations\n",
    "        all_recommendations[title] = top_movies\n",
    "\n",
    "    return all_recommendations\n",
    "\n",
    "# Example usage\n",
    "all_recommendations = get_recommendations_for_all(merged_data,ratings)\n",
    "print(all_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a8cd54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
